{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Cost Function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log (h_\\theta(x^{(i)})) - (1-y^{(i)})\\log (1-h_\\theta(x^{(i)})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Want $ \\min_\\theta J(\\theta): $\n",
    "    \n",
    "Repeat   {\n",
    "    $$ \\theta_j :=  \\theta_j  - \\alpha \\sum_{i=1}^{m} h_\\theta(x^{(i)} - y^{(i)}) x_j^{(i)} $$\n",
    "    (simultaneously update all $\\theta_j$)\n",
    "    \n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_\\theta(x) = \\frac{1}{1+ e^{-\\theta^T{x}} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized implementation: \n",
    "    $$h = g(H\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{m} (-y^T \\log(h) - (1-y)^T \\log(1-h))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta := \\theta - \\frac{\\alpha}{m} X^T (g(X\\theta) - \\overrightarrow{y}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from  scipy.optimize import minimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ex2data1.txt', header = None)\n",
    "data = np.array(data)\n",
    "X = data[:, [0,1]] \n",
    "y = data[:, [2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g(z) = \\frac{1}{1+ e^{-z}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$h_\\theta(x)= g(\\theta^T x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    sigm = 1.0/(1.0+np.exp(-z))\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.insert(X, 0, values=1, axis=1)\n",
    "theta = np.zeros((X.shape[1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function and gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Cost Function\n",
    "\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log (h_\\theta(x^{(i)})) - (1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized implementation: \n",
    "    $$h = g(H\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = \\frac{1}{m} (-y^T \\log(h) - (1-y)^T \\log(1-h))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_theta = np.zeros((X.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, x, y):   \n",
    "    m = len(y)\n",
    "    h_theta = g(x.dot(theta))\n",
    "    J = (1.0/m)* (((-y).transpose()).dot(np.log(h_theta)) - (1.0 -y.transpose()).dot(np.log(1.0-h_theta)))\n",
    "    grad = grad = (1.0/m)* x.transpose().dot(h_theta - y)    \n",
    "    #return J, grad\n",
    "    print 'Cost at theta:', str(J[0,0])\n",
    "    print 'Gradient at theta:','\\n', str(grad[0,0]),'\\n', str(grad[1,0]),'\\n', str(grad[2,0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta: 0.69314718056\n",
      "Gradient at theta: \n",
      "-0.1 \n",
      "-12.0092165893 \n",
      "-11.2628422055\n"
     ]
    }
   ],
   "source": [
    "costFunction(in_theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_theta= np.array([[-24], [0.2], [0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta: 0.218330193827\n",
      "Gradient at theta: \n",
      "0.0429029948995 \n",
      "2.56623411551 \n",
      "2.64679737108\n"
     ]
    }
   ],
   "source": [
    "costFunction(test_theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostFunction(theta, x, y):\n",
    "    m = len(y)\n",
    "    h_theta = g(x.dot(theta))\n",
    "    J = (1.0/m)* (((-y).transpose()).dot(np.log(h_theta)) - (1.0 -y.transpose()).dot(np.log(1.0-h_theta)))\n",
    "    J = np.float64(J)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " def Gradient(theta, x, y):\n",
    "    m = len(y)\n",
    "    n = x.shape[1]\n",
    "    theta = theta.reshape((n,1))\n",
    "    h_theta = g(x.dot(theta))\n",
    "    grad = (1.0/m)* (x.transpose().dot(h_theta - y)) \n",
    "    return grad.flatten()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta: 0.203497701589 \n",
      "Theta: [-25.16131866   0.20623159   0.20147149]\n"
     ]
    }
   ],
   "source": [
    "Result = opt(fun = CostFunction, x0 = in_theta, args = (X, y), method = 'TNC', jac = Gradient, options ={'maxiter':400})\n",
    "theta = Result.x\n",
    "print'Cost at theta:',Result.fun, '\\n', 'Theta:', Result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a student with score 45 on exam 1\n",
      " and score 85 on exam 2 we predict an admission probability of  0.776290624348\n"
     ]
    }
   ],
   "source": [
    "prob = g(np.array([1, 45, 85]).dot(theta))\n",
    "print 'For a student with score 45 on exam 1\\n and score 85 on exam 2 we predict an admission probability of ', prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, x):    \n",
    "    m = X.shape[0]\n",
    "    p = np.zeros((m,1))\n",
    "    n = X.shape[1]\n",
    "    theta = theta.reshape((n,1))\n",
    "    h_theta = g(X.dot(theta))    \n",
    "    for i in range(0, h_theta.shape[0]):\n",
    "        if h_theta[i] > 0.5:\n",
    "            p[i, 0] = 1\n",
    "        else:\n",
    "            p[i, 0] = 0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 89.0\n"
     ]
    }
   ],
   "source": [
    "p = predict(theta, X)\n",
    "print 'Train Accuracy:', (y[p == y].size / float(y.size)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from  scipy.optimize import minimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ex2data2.txt', header = None)\n",
    "data = np.array(data)\n",
    "X = data[:, [0,1]] \n",
    "y = data[:, [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size = X[:,0].shape[0]\n",
    "\n",
    "X1 = X[:,0]\n",
    "X1 = X1.reshape(x_size, 1)\n",
    "\n",
    "X2 = X[:,1]\n",
    "X2 = X2.reshape(x_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1, X2):\n",
    "    degree = 6\n",
    "    out = np.ones((X1.shape[0], 1))\n",
    "    for i in range(1, degree+1):\n",
    "        for j in range (0, i+1):\n",
    "            out1 = np.power(X1, (i-j))\n",
    "            out2 = np.power(X2, j)\n",
    "            out = np.concatenate((out,out1*out2),axis=1)\n",
    "       \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mapFeature(X1, X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Cost function and Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized Logistic Regression Cost Function\n",
    "\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log (h_\\theta(x^{(i)})) - (1-y^{(i)})\\log (1-h_\\theta(x^{(i)}))] +  \\frac{\\lambda}{2m}\\sum_{j=1}^{n} \\theta_j^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_theta = np.zeros((X.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    sigm = 1.0/(1.0+np.exp(-z))\n",
    "    return sigm\n",
    "\n",
    "def CostFunction(theta, x, y):\n",
    "    m = len(y)\n",
    "    h_theta = g(x.dot(theta))\n",
    "    J = (1.0/m)* (((-y).transpose()).dot(np.log(h_theta)) - (1.0 -y.transpose()).dot(np.log(1.0-h_theta)))\n",
    "    J = np.float64(J)\n",
    "    return J\n",
    "\n",
    "def Gradient(theta, x, y):\n",
    "    m = len(y)\n",
    "    n = x.shape[1]\n",
    "    theta = theta.reshape((n,1))\n",
    "    h_theta = g(x.dot(theta))\n",
    "    grad = (1.0/m)* (x.transpose().dot(h_theta - y))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, x, y, lamb):\n",
    "    m = len(y)\n",
    "    J = CostFunction(theta, x, y)\n",
    "    sum_theta = (theta[1:]**2).sum()\n",
    "    J = J +lamb/(2*m)*sum_theta\n",
    "    return J\n",
    "\n",
    "def GradReg(theta, x, y, lamb):\n",
    "    m = len(y)\n",
    "    n = x.shape[1]\n",
    "    theta = theta.reshape((n,1))\n",
    "    grad = Gradient(theta, X, y)\n",
    "    grad[1:,:] =grad[1:,:]+(lamb/m)*theta[1:,:]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial theta (zeros): 0.69314718056 \n",
      "Gradient at initial theta (zeros) - first five values only: [[  8.47457627e-03]\n",
      " [  1.87880932e-02]\n",
      " [  7.77711864e-05]\n",
      " [  5.03446395e-02]\n",
      " [  1.15013308e-02]]\n"
     ]
    }
   ],
   "source": [
    "cost = costFunctionReg(in_theta, X, y, lamb)\n",
    "grad = GradReg(in_theta, X, y, lamb)\n",
    "print 'Cost at initial theta (zeros):', cost, '\\n', 'Gradient at initial theta (zeros) - first five values only:', grad[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_theta = np.ones((X.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at test theta: 2.020441535 \n",
      "Gradient at test theta - first five values only: [[ 0.34604507]\n",
      " [ 0.07660616]\n",
      " [ 0.11004999]\n",
      " [ 0.14211702]\n",
      " [ 0.00743991]]\n"
     ]
    }
   ],
   "source": [
    "cost = CostFunction(test_theta, X, y)\n",
    "grad = Gradient(test_theta, X, y)\n",
    "print 'Cost at test theta:', cost, '\\n', 'Gradient at test theta - first five values only:', grad[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at theta: 0.250236672827 \n",
      "Theta: [   13.47701351    19.97645483    12.24521409  -136.71859538   -69.17157502\n",
      "   -47.0636926   -151.0565139   -165.98761607   -86.99502279   -11.52689396\n",
      "   480.93353632   445.19456552   535.50346536   223.30098962    48.73230863\n",
      "   262.45927299   460.44954642   524.55822431   304.87257986    83.01116309\n",
      "    -4.45569691  -558.43698419  -806.53951448 -1189.44213475  -880.87932779\n",
      "  -670.60722854  -232.15365936   -18.16217856]\n"
     ]
    }
   ],
   "source": [
    "Result = opt(fun = costFunctionReg, x0 = in_theta, args = (X, y, lamb), method = 'TNC', jac = GradReg, options ={'maxiter':400})\n",
    "theta = Result.x\n",
    "print'Cost at theta:',Result.fun, '\\n', 'Theta:', theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, x): \n",
    "    m = X.shape[0]\n",
    "    p = np.zeros((m,1))\n",
    "    n = X.shape[1]\n",
    "    theta = theta.reshape((n,1))\n",
    "    h_theta = g(X.dot(theta))\n",
    "    for i in range(0, h_theta.shape[0]):\n",
    "        if h_theta[i] > 0.5:\n",
    "            p[i, 0] = 1\n",
    "        else:\n",
    "            p[i, 0] = 0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 88.1355932203\n"
     ]
    }
   ],
   "source": [
    "p = predict(theta, X)\n",
    "print 'Train Accuracy:', (y[p == y].size / float(y.size)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
